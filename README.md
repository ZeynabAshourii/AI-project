# ðŸš€ UNet-RoadMapper
This code is a complete project for road image segmentation using different variants of the U-Net architecture. At the beginning, the dataset is mounted from Google Drive, extracted from a compressed .zip file, and organized into training and validation folders. Images and their corresponding masks are read, resized to a fixed resolution of 256x256, converted into NumPy arrays, and split into training and validation sets. To verify correctness, a few samples of images and their masks are visualized. A custom PyTorch Dataset class is implemented to handle preprocessing: images are converted to tensors and normalized to the range [-1, 1], while masks are binarized into values of 0 and 1. These datasets are then wrapped in DataLoader objects to create batches for efficient training and evaluation.

The project implements three segmentation models: basic U-Net, Attention U-Net, and Residual Attention U-Net. The U-Net consists of an encoder that progressively downsamples the image using convolutional blocks and max pooling to extract features, a bottleneck layer that captures compressed high-level features, and a decoder that upsamples the features back to the original resolution. Skip connections are used to concatenate encoder and decoder features. Attention U-Net extends this by inserting an Attention Block before each skip connection, filtering out irrelevant information and allowing only the most important encoder features to pass through. Residual Attention U-Net further enhances this by replacing convolutional blocks with Residual Blocks, which add shortcut connections inside each block to improve gradient flow and stabilize training. This combination of residual connections and attention mechanisms makes the model more accurate and robust for segmentation tasks.

For training, three loss functions are defined: Dice Loss, which measures pixel-level overlap between prediction and ground truth; IoU Loss, which evaluates region-based overlap; and Binary Cross-Entropy Loss, which penalizes pixel-wise classification errors. These are combined into an Overall Loss that balances the strengths of all three metrics. During training, the model is optimized with Adam, and after each epoch it is evaluated on the validation set. Performance is measured using IoU score and Dice score, and the best-performing model is saved automatically.

Finally, the saved Residual Attention U-Net model is loaded and tested on validation data. Predictions are generated, thresholded at 0.5 to obtain binary masks, and displayed alongside the original images and ground truth masks for qualitative comparison.

In summary, this project covers the entire workflow of a deep learning segmentation pipeline: dataset preparation and preprocessing, model design with advanced architectures, implementation of loss functions tailored for segmentation, training and validation with performance monitoring, and visualization of predictions. It provides a strong framework for road segmentation and can easily be adapted to other image segmentation tasks.

---

# ðŸ¤– SAC-Gymnasium-Agent
This code is a complete implementation of the Soft Actor-Critic (SAC) reinforcement learning algorithm, designed to train an autonomous agent to master the HalfCheetahBulletEnv-v0 simulation environment. SAC is an advanced off-policy algorithm renowned for its sample efficiency and stability, which it achieves by maximizing not only cumulative reward but also the entropy of the policy, thereby encouraging exploration and robust behavior. The code begins by defining a utility function for plotting the learning curve, which visualizes the agent's score per episode alongside a rolling average to track progress over time. The core functionality is structured into several specialized classes. The ReplayBuffer class serves as the agent's memory, efficiently storing and managing experiencesâ€”each consisting of a state, action, reward, next state, and termination flagâ€”in large NumPy arrays. It implements a circular buffer strategy and provides a method to randomly sample mini-batches of these experiences, which is crucial for breaking the temporal correlation between consecutive data points during training.

The neural network architecture is defined through three distinct PyTorch classes. The Critic network estimates the Q-value of state-action pairs; it takes the state and action as concatenated inputs, processes them through fully connected layers with ReLU activations, and outputs a single Q-value. Two separate Critic networks are used to prevent overestimation. The Actor network represents the agent's policy by parameterizing a probability distribution over actions. For a given state, it outputs the mean and standard deviation of a Gaussian distribution. Its key method, sample_normal, uses the reparameterization trick to sample actions in a way that allows gradients to flow back through the stochastic process, applies a tanh transformation to constrain actions to the environment's limits, and computes the corresponding log probability. The Value network estimates the value of a state under the current policy, forming the foundation for calculating update targets.

The Agent class acts as the central orchestrator, integrating all components and managing the learning process. During initialization, it creates instances of the replay buffer, the actor, the two critics, the value network, and a target value network. The target network is initially a clone of the value network and is updated slowly to enhance learning stability. The agent interacts with the environment using its choose_action method, which can operate in either a deterministic or stochastic mode. Experiences collected from these interactions are stored in the replay buffer. The learn method is the algorithmic core: it periodically samples a batch of experiences, converts them to PyTorch tensors, and performs a series of calculations to update all networks. The value network is updated to minimize the mean-squared error against a target derived from the minimum Q-value from the two critics minus an entropy term. The actor is updated to maximize the expected Q-value (again from the minimum of the two critics) plus the entropy bonus. Finally, the two critics are updated to minimize the error between their current Q-value estimates and a target computed using the reward and the target value network's estimate of the next state's value. The target value network's parameters are then softly updated towards the main value network's parameters.

The execution section configures the hyperparameters, creates the Gymnasium environment, and wraps it for video recording if specified. The SAC agent is instantiated with these parameters. The main training loop runs for a predetermined number of episodes. In each episode, the agent resets the environment and interacts with it step-by-step, storing experiences and triggering learning updates. The episode's total reward (score) is recorded, and a running average is maintained. The best-performing model, based on this average score, is saved to disk throughout the training process. After training concludes, the learning curve is plotted to visualize the agent's performance improvement, providing a clear summary of the training efficacy and the agent's learning progression. This entire pipeline demonstrates a modern, production-ready deep reinforcement learning system built with PyTorch.

---
Zeynab Ashouri : https://drive.google.com/drive/folders/1zrrnnqH4FaFzaRtJ9WLJA2WvNfsGJAtm?usp=drive_link

Sogol Zamanian : (z)

Phase 1:  
Zeynab Ashouri: Answering Question 1 at the beginning of the video (ai-1)  +  line-by-line explanation of the code up to the end of the "Complete UNet Structure" section in the rest of the video.  
Sogol Zamanian: Line-by-line explanation of the rest of the code  +  answering Questions 2, 3, and 4 at the end of the video (x).  

Phase 2:  
Zeynab Ashouri: Answering Question 1 at the beginning of the video (ai-2)  +  line-by-line explanation of the code up to the end of the "Neural Networks" section in the rest of the video.  
Sogol Zamanian: Line-by-line explanation of the rest of the code  +  answering Questions 2, 3, and 4 at the end of the video (y).
